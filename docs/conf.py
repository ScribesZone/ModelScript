# -*- coding: utf-8 -*-


import sys
import os

#--------------------------------------------------------------------------
# >+JFE
#--------------------------------------------------------------------------

# Add the root to the path, necessary so that autodoc find packages
sys.path.insert(0, os.path.abspath(os.path.join('..')))

import os

# import sys
# import mock
#
# def addMocksForMissingModuleFile():
#     missing_modules_file = os.path.join(
#         os.path.dirname(os.path.realpath(__file__)),
#         'missing-modules.txt')
#     missing_modules = [line.strip() for line in open(missing_modules_file)]
#     for module_name in missing_modules:
#         sys.modules[module_name] = mock.Mock()
#
#
# addMocksForMissingModuleFile()

#PYMODELIO_STARTUP = os.path.join(os.path.expanduser("~"), ".modelio",
#                                 "pymodelio_startup.py")
#execfile(PYMODELIO_STARTUP)
#--------------------------------------------------------------------------
# <+JFE
#--------------------------------------------------------------------------







# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration ------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    # 'sphinx.ext.autodoc',
    # 'sphinx.ext.napoleon',
    # 'sphinx.ext.autosummary',
    # 'sphinx.ext.viewcode',
     # 'sphinx.ext.doctest',
    # 'sphinx.ext.intersphinx',
    # 'sphinx.ext.todo',
    # 'sphinx.ext.coverage',
    # 'sphinx.ext.ifconfig',
]

# autodoc_member_order = 'bysource'
# autoclass_content = 'both'
# autodoc_default_flags = ['undoc-members']
# autosummary_generate = True
# napoleon_use_param = True



# Add any paths that contain templates here, relative to this directory.
templates_path = ['templates']

# The suffix of source filenames.
# source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the projects.
project = u'ModelScript'
copyright = u'2018, escribis'

# The version info for the projects you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.8.1'
# The full version, including alpha/beta/rc tags.
release = version # '0.0.3'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# exclude_patterns = ['.build']

# The reST default role (used for this markup: `text`) to use for all
# documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ----------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.

#--------------------------------------------------------------------------
# >+JFE
#--------------------------------------------------------------------------
# html_theme = 'default'

# read the docs theme.
# see https://github.com/snide/sphinx_rtd_theme

# on_rtd is whether we are on readthedocs.org
on_rtd = os.environ.get('READTHEDOCS', None) == 'True'
if not on_rtd:  # only import and set the theme if we're building docs locally
    import sphinx_rtd_theme
    html_theme = 'sphinx_rtd_theme'
    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]
# otherwise, readthedocs.org uses their theme by default, so no need to specify it


# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {
    'display_version': False}

#--------------------------------------------------------------------------
# <+JFE
#--------------------------------------------------------------------------

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<projects> v<release> documentation".
# html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# html_static_path = ['_static']

# Add any extra paths that contain custom files (such as robots.txt or
# .htaccess) here, relative to this directory. These files are copied
# directly to the root of the documentation.
#html_extra_path = []

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
html_show_sphinx = False

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
html_show_copyright = False

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None



# -- Options for LaTeX output ---------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
  ('index', 'PyUseOCL.tex', u'PyUseOCL Documentation',
   u'megaplanet', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output ---------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'pyuseocl', u'PyUseOCL Documentation',
     [u'escribis'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output -------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'pyuseocl', u'PyUseOCL Documentation',
   u'escribis', 'PyUseOCL', 'One line description of projects.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

# from pygments.lexer import RegexLexer
from pygments import lexer
from pygments import token
from sphinx.highlighting import lexers
import re

"""
Comment
    Multiline
    Special
"""

#TODO:3 review the highlighting method belo
#  in particular the inheritance mechanisms and the
#  way to deal with  common keywords (import, from, model)

# This list is important to deal with "import XXX model from"
# where xxx is one of the keyword below.

AllModelKeywords="""
    glossary
    track
    class
    object
    relation
    participant
    usecase
    aui
    task
    permission
    scenario    
""".split()

ModelScriptKeywords="""
    import
    from
    model
""".split() + AllModelKeywords

class ModelScriptLexer(lexer.RegexLexer):
    # see http://pygments.org/docs/lexerdevelopment/
    flags=re.MULTILINE | re.UNICODE
    # "keywords is added. not in the pygment framework.
    # keywords=\
    #     """
    #     mo XXX del
    #     import
    #     """.split()

    tokens = {
        'root': [

            (lexer.words(ModelScriptKeywords, suffix=r'\b'),
             token.Keyword),

            # "class" token for what follows "model"
            ('^(\w+)( +)(model)( *)(\w*)',
             lexer.bygroups(
                 token.Keyword,
                 lexer.Text,
                 token.Keyword,
                 lexer.Text,
                 token.Name.Class
             )),

            (r' *\|', token.Comment.Multiline, 'doc'),
            (r'//.*\n', token.Comment.Special),
            (r'--.*\n', token.Comment.Special),
            (r'".*"', token.Literal.String),
            (r'\'.*\'', token.Literal.String),
            (r'\-?[0-9]+(\.[0-9]+)?', token.Literal.Number),
            (r' ', token.Text),
            (r'[.,{}:;<>()=+\-*\[\]!]', token.Text)

        ],
        'doc': [
            ('`\w+`', token.Name.Function),   # Name.Entity
            ('.', token.Comment)
        ]
    }

#--------------------------------------------------------------------------
#  GlossaryScript Lexer
#--------------------------------------------------------------------------

GlossaryScriptKeywords= """
    synonyms
    synonym
    inflections
    inflection
    translations
    translation
    expansion
    package
    """.split()

class GlossaryScriptLexer(ModelScriptLexer):
    name = 'GlossaryScript'

    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            lexer.inherit,
            ('(package)( +)(\w+)',
                lexer.bygroups(
                    token.Keyword,
                    lexer.Text,
                    token.Name.Class)),
            (lexer.words(GlossaryScriptKeywords, suffix=r'\b'), token.Keyword),
            ('^\w+', token.Name.Class),
            (r'\w+', token.Name),
        ]
    }

lexers['GlossaryScript'] = GlossaryScriptLexer(
    startinline=True,
    encoding = 'utf-8')

# --------------------------------------------------------------------------
#  ParticipantScript Lexer
# --------------------------------------------------------------------------

TrackScriptKeywords = """
    track
    
    question        question
    hypothesis      hypothese
    decision        decision
    problem         probleme
    
    github
    priority
    status
    date
    who
    conclusion
    history
    
    """.split()


class TrackScriptLexer(ModelScriptLexer):
    name = 'TrackScript'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            lexer.inherit,
            ('^(Track)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            ('(extension)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            (lexer.words(TrackScriptKeywords, suffix=r'\b'),
             token.Keyword),
            (r'\w+', token.Name),
        ]
    }


lexers['TrackScript'] = TrackScriptLexer(
    startinline=True,
    encoding='utf-8')

#--------------------------------------------------------------------------
#  ClassScript1 Lexer
#--------------------------------------------------------------------------

ClassScript1Keywords= """
    enum
    abstract
    class
    end
    attributes
    operations
    
    derived
    init
    
    Set
    Bag
    OrderedSet
    Sequence
    
    association
    between
    role
    associationclass
    $
    inv
    pre 
    post
    begin
    new
    declare
    
    """.split()

class ClassScript1Lexer(ModelScriptLexer):
    name = 'ClassScript1'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            (r'^ *--\|', token.Comment.Multiline, 'doc'),
            (r'--@.*\n', token.Comment.Multiline),

            lexer.inherit,

            ('(enum|class|association|associationclass)( +)(\w+)',
                lexer.bygroups(
                    token.Keyword,
                    lexer.Text,
                    token.Name.Class)),
            (lexer.words(ClassScript1Keywords, suffix=r'\b'), token.Keyword),
            (r'\w+', token.Name),
        ]
    }

lexers['ClassScript1'] = ClassScript1Lexer(
    startinline=True,
    encoding = 'utf-8')




# --------------------------------------------------------------------------
#  ObjectScript1 Lexer
# --------------------------------------------------------------------------

ObjectScript1Keywords = """
    create
    new
    insert
    into
    between
    """.split()


class ObjectScript1Lexer(ModelScriptLexer):
    name = 'ObjectScript1'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            (r'^ *--\|', token.Comment.Multiline, 'doc'),
            (r'--@.*\n', token.Comment.Multiline),

            lexer.inherit,

            (lexer.words(ObjectScript1Keywords, suffix=r'\b'), token.Keyword),
            (r'\w+', token.Name),
        ]
    }


lexers['ObjectScript1'] = ObjectScript1Lexer(
    startinline=True,
    encoding='utf-8')




# --------------------------------------------------------------------------
#  UsecaseScript Lexer
# --------------------------------------------------------------------------

UsecaseScriptKeywords = """
    
    interactions
    a
    an
    can 
    
    actor
    
    usecase
    primary
    secondary
    adhoc
    persona
    volume
    frequency
    description
    goal
    precondition
    trigger
    postcondition
    risk
    low
    high
    medium
    flow
    extension
    at
    when
    """.split()


class UsecaseScriptLexer(ModelScriptLexer):
    name = 'UsecaseScript'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            lexer.inherit,
            ('^(usecase)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            ('(extension)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            (lexer.words(UsecaseScriptKeywords, suffix=r'\b'), token.Keyword),
            (r'\w+', token.Name),
        ]
    }


lexers['UsecaseScript'] = UsecaseScriptLexer(
    startinline=True,
    encoding='utf-8')

# --------------------------------------------------------------------------
#  ParticipantScript Lexer
# --------------------------------------------------------------------------

ParticipantScriptKeywords = """
    participant
    
    actor
    stakeholder
    role
    team
    person
    persona
    adhoc
    
    name
    trigram
    portrait
    
    attitudes
    
    aptitudes
    education
    languages
    language
    age
    disabilities
    skills
    motivations
    learning
    ability
    
    motivations
    why
    level
    kind
    
    skills
    culture
    modalities
    environments
    

    """.split()


class ParticipantScriptLexer(ModelScriptLexer):
    name = 'ParticipantScript'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            lexer.inherit,
            ('^(Participant)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            ('(extension)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            (lexer.words(ParticipantScriptKeywords, suffix=r'\b'),
             token.Keyword),
            (r'\w+', token.Name),
        ]
    }


lexers['ParticipantScript'] = ParticipantScriptLexer(
    startinline=True,
    encoding='utf-8')

# --------------------------------------------------------------------------
#  AUIScript Lexer
# --------------------------------------------------------------------------

AUIScriptKeywords = """
    
    aui
    space
    concepts
    links
    back
    to
    transformation
    from
    rule
    """.split()


class AUIScriptLexer(ModelScriptLexer):
    name = 'AUIScript'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            lexer.inherit,
            (lexer.words(AUIScriptKeywords, suffix=r'\b'), token.Keyword),
            (r'\w+', token.Name),
        ]
    }


lexers['AUIScript'] = AUIScriptLexer(
    startinline=True,
    encoding='utf-8')




# --------------------------------------------------------------------------
#  ScenarioScript Lexer
# --------------------------------------------------------------------------

ScenarioScript1Keywords = """
    create
    new
    insert
    into
    between
    open
    """.split()


class ScenarioScript1Lexer(ModelScriptLexer):
    name = 'ScenarioScript1'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            (r'^ *--\|', token.Comment.Multiline, 'doc'),
            (r'--@ *usecase.*\n', token.Comment.Special),
            (r'--@ *context.*\n', token.Comment.Special),
            (r'--@.*\n', token.Generic.Deleted),

            lexer.inherit,

            (lexer.words(ScenarioScript1Keywords, suffix=r'\b'), token.Keyword),
            (r'\w+', token.Name),
        ]
    }


lexers['ScenarioScript1'] = ScenarioScript1Lexer(
    startinline=True,
    encoding='utf-8')




# --------------------------------------------------------------------------
#  RelationScript Lexer
# --------------------------------------------------------------------------

RelationScriptKeywords = """

    relation
    intention
    in
    examples
    <=>
    
    constraints
    
    dom
    String      s
    Real        r
    Boolean     b
    Integer     i
    Date        d
    DateTime    dt
    Time        t
    key
    prime
    ffd
    1NF
    2NF
    3NF
    BCNF
     
    transformation
    from
    rules
    rule
    
    dataset
    query
    """.split()


class RelationScriptLexer(ModelScriptLexer):
    name = 'RelationScript'
    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            lexer.inherit,
            ('^(relation)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            ('(extension)( +)(\w+)',
             lexer.bygroups(token.Keyword, lexer.Text, token.Name.Class)),
            (lexer.words(RelationScriptKeywords, suffix=r'\b'),
             token.Keyword),
            (r'\w+', token.Name),
        ]
    }


lexers['RelationScript'] = RelationScriptLexer(
    startinline=True,
    encoding='utf-8')


#--------------------------------------------------------------------------
#  PermissionScript Lexer
#--------------------------------------------------------------------------

PermissionScriptKeywords= """
    can             peut
    create          creer
    read            lire
    update          modifier
    delete          detruire
    
    C       C
    R       L
    CR      CL
    U       M
    CU      CM
    RU      LM
    CRU     CLM
    D       D
    CD      CD
    RD      LD
    CRD     CLD
    UD      MD
    CUD     CMD
    RUD     LMD
    CRUD    CLMD
    """.split()

class PermissionScriptLexer(ModelScriptLexer):
    name = 'PermissionScript'

    # flags = re.MULTILINE | re.UNICODE
    tokens = {
        'root': [
            lexer.inherit,
            (lexer.words(PermissionScriptKeywords, suffix=r'\b'), token.Keyword),
            (r'\w+', token.Name),
        ]
    }

lexers['PermissionScript'] = PermissionScriptLexer(
    startinline=True,
    encoding = 'utf-8')